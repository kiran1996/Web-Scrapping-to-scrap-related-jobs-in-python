{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67993309",
   "metadata": {},
   "source": [
    "In this notebook I am aiming to get all the data science related jobs available in Melbourne location from Seek.\n",
    "All the matching skills will be displayed at the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81c0eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from xlsxwriter import workbook\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485912a4",
   "metadata": {},
   "source": [
    "After importing the libraries I have used for loop to iterate through first 20 pages of the job posting for Data analyst position and Melbourne and inner suburbs locations. This gives all the urls for the first 20 pages and requested the html_text to be read from requests.get(url) and read only text from it.\n",
    "\n",
    "We are going to import details of each jobs with date posted, name of the company, position title, Link for the job posting and details of the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7dacb759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This dataframe is created to store the jobs from first 20 pages of the Seek job board.\n",
    "output_column_names = {'Company_name': str(), 'Job_title': str(), 'Job_Link': str(), 'Time_Posted': str(), 'Job Details': str()}\n",
    "data1 = pd.DataFrame(output_column_names, index=[])\n",
    "\n",
    "#Now comes the for loop to loop through first 20 pages of the webpage.\n",
    "for i in range(1,20):\n",
    "    url = 'https://www.seek.com.au/data-analyst-jobs/in-Melbourne-CBD-&-Inner-Suburbs-Melbourne-VIC?page={}'.format(i)\n",
    "    html_text = requests.get(url).text\n",
    "    #Now we will read all the html texts available in each page.\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    #Below is the class for each specific job postings and further reads will be done through that only.\n",
    "    jobs = soup.find_all('div', class_ = 'yvsb870 v8nw070 v8nw072')\n",
    "    data = []\n",
    "    for job in jobs:\n",
    "        #There are two featured jobs in each page with no date posting so we will remove those using the below lines.\n",
    "        node = job.find(lambda tag: tag.name == 'span' and \n",
    "                                        tag.get('class') == ['yvsb870'])\n",
    "        if node is not None:\n",
    "            #Extraction of posting date is as below.\n",
    "            date_posted = job.find(lambda tag: tag.name == 'span' and \n",
    "                                            tag.get('class') == ['yvsb870']).text\n",
    "            #As this needs to be done for the new job postings we are now extracting data only which consists ago in their.\n",
    "            #So, below filter will remove out job postings with dates such as 30th January which is like one month in the past and we do not want those postings.\n",
    "            if re.search('ago', date_posted):\n",
    "                #Now in the date_posted we will only extract text containing time of posting. Like 30m.\n",
    "                date_posted = date_posted.split()\n",
    "                date_posted = date_posted[0]\n",
    "                #Extraction of company name is done through below.\n",
    "                company_name = job.find('a', class_ = '_842p0a0').text\n",
    "                #Now we will extract Job title using below code.\n",
    "                Job_title = job.find('a', class_ = '_1tmgvw5 _1tmgvw7 _1tmgvwa _1tmgvwb _1tmgvwe yvsb870 yvsb87f _14uh994h').text\n",
    "                #Below code is for Job Links extraction. So, one class does have lots of links available we only extract the job link for specific job.\n",
    "                Job_link1 = []\n",
    "                Job_link_Attr = job.find('h3', class_ = 'yvsb870 _14uh9944u _1cshjhy0 _1cshjhyl _1d0g9qk4 _1cshjhys _1cshjhy21')\n",
    "                for link in Job_link_Attr.find_all('a'):\n",
    "                    Job_link1.append(link.get('href'))\n",
    "                Job_link = Job_link1[0]\n",
    "                #Also, the job link is not in the required format. So, adding the headers at first.\n",
    "                Job_link = 'https://www.seek.com.au' + Job_link\n",
    "                #Details of the job is done through below code.\n",
    "                Job_Skills = job.find_all('span', class_ = 'yvsb870 _14uh9944u _1cshjhy0 _1cshjhy1 _1cshjhy21 _1d0g9qk4 _1cshjhy7')\n",
    "                Job_Skills1 = []\n",
    "                for i in range(3,len(Job_Skills)):\n",
    "                    Job_Skills1.append(Job_Skills[i].text)\n",
    "                Job_Skills1 = '\\n'.join(Job_Skills1)\n",
    "                data.append([company_name, Job_title, Job_link, date_posted, Job_Skills1])\n",
    "    data = pd.DataFrame(data, columns=output_column_names)\n",
    "    #data1 = data1.append(data)\n",
    "    data1 = pd.concat([data1, data])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0986dc95",
   "metadata": {},
   "source": [
    "Now that we have jobs for all the dates available we will now convert the job posting time to seconds and only save the jobs which are posted in recent 5 days and store the excel in the local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c411df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2 = data1[data1[\"Time_Posted\"].str.contains(\"h\", \"m\", \"s\", \"d\")]\n",
    "\n",
    "seconds_per_unit = {\"s\": 1, \"m\": 60, \"h\": 3600, \"d\": 86400, \"w\": 604800}\n",
    "\n",
    "def convert_to_seconds(s):\n",
    "    return int(s[:-1]) * seconds_per_unit[s[-1]]\n",
    "#Second = []\n",
    "#for i in data1:\n",
    "#    try:\n",
    "#        Second.append(data1[\"Time_Posted\"][i].apply(convert_to_seconds))\n",
    "data1[\"Seconds_Posted\"] = data1[\"Time_Posted\"].apply(convert_to_seconds)\n",
    "data2 = data1.loc[data1['Seconds_Posted'] < 433000]\n",
    "data2.to_excel(\"C:/Users/naiki/Downloads/Jobs1.xlsx\", sheet_name=\"Results\", engine='xlsxwriter', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
